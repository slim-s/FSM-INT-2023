{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e545491e-4d74-4a3d-8908-a7e501440ae6",
   "metadata": {},
   "source": [
    "# Predictive maintenance of Lathe machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e819db72-d3b2-4eca-953f-6d75dc685401",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Importing important libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419d5f7a-1cbc-4f98-9e84-f36aae31527b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from scipy import signal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443be3fd-9b2d-4385-9f64-70a689569155",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ea6834-0364-4056-ad83-48bdb30bfcbf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Importing Vibration Data and converting it to proper Time-series format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3abe66-1a7e-441c-a82d-08c041a3ccb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Experiment details\n",
    "exp = pd.read_excel(\"Data/Experiment Summary.xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6196d3b-5b70-4ccf-9c2c-0a31706c5e42",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframes = []  # To store the imported data from each file\n",
    "\n",
    "for i in range(1, 61):\n",
    "    file = f\"Data/{i}.xlsx\"\n",
    "    df = pd.read_excel(file)  # Use pd.read_excel() for Excel files\n",
    "    df = df.dropna(axis='columns', how='all')  \n",
    "    df = df.dropna(axis='rows', how='all') \n",
    "    df.columns = ['Time', 'X', 'Y', 'Z']\n",
    "    df = df.iloc[1:]  # Exclude the original header row from the data\n",
    "    df['Time'] = pd.to_datetime(df['Time'], unit='s').dt.time  # Convert 'Time' column to datetime\n",
    "    \n",
    "    # Add experiment details from exp dataframe based on experiment number\n",
    "    experiment_number = i  # Or any other way to determine the experiment number\n",
    "    experiment_row = exp[exp['Experiment'] == experiment_number]\n",
    "    if not experiment_row.empty:\n",
    "        for column in experiment_row.columns[1:]:\n",
    "            df[column] = experiment_row[column].values[0]\n",
    "    \n",
    "    dataframes.append(df)\n",
    "    \n",
    "    print(i)  # To keep an eye on progress\n",
    "\n",
    "\n",
    "for i in range(len(dataframes)):\n",
    "    dataframes[i]['X'] = pd.to_numeric(dataframes[i]['X'], errors='coerce')\n",
    "    dataframes[i]['Y'] = pd.to_numeric(dataframes[i]['Y'], errors='coerce')\n",
    "    dataframes[i]['Z'] = pd.to_numeric(dataframes[i]['Z'], errors='coerce')\n",
    "    dataframes[i]['Magnitude'] = np.sqrt(dataframes[i]['X']**2 + dataframes[i]['Y']**2 + dataframes[i]['Z']**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f7a137-f093-4cd9-abee-b9aac725c1b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "merged_df['Time'] = [t.hour * 3600 + t.minute * 60 + t.second + t.microsecond / 1e6 for t in merged_df['Time']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67520fc-c4c0-4b0e-babb-c54c87fcec1b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Exploratory Data Analyis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb1eec8-1489-4df8-80c2-a8e4cad56517",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Compute the correlation matrix\n",
    "corr=merged_df.corr(numeric_only=True)\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(corr, square=True,annot=True,cmap='YlGnBu',linecolor=\"black\")\n",
    "plt.title('Correlation between features')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdee6801-8fcb-4338-8932-30fe6229f430",
   "metadata": {},
   "source": [
    "#### Following are the observations from correlation heat map:\n",
    "    1) X_acceleration is the most prominent in the magnitude calculated.\n",
    "    2) Surface roughness and feed rate are positively correlated \n",
    "    3) Surface roughness and RPM of spindle are negatively correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ae8d28-ab16-4fa9-bbe0-ed0f041f6043",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400df76b-b037-4cf3-8f39-9ca3882b3cd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculating PSD\n",
    "\n",
    "def calculate_psd(dataframe, fs=1000, w=10):\n",
    "    time = dataframe['Time']\n",
    "    mag = dataframe['Magnitude']\n",
    "    x = dataframe['X']\n",
    "    y = dataframe['Y']\n",
    "    z = dataframe['Z']\n",
    "    \n",
    "    # Convert time values to seconds\n",
    "    time_seconds = [(t.hour * 3600 + t.minute * 60 + t.second + t.microsecond / 1e6) for t in time]\n",
    "\n",
    "    # Apply Hanning window function\n",
    "    window = np.hanning(len(time_seconds))\n",
    "    x_windowed = x * window\n",
    "    y_windowed = y * window\n",
    "    z_windowed = z * window\n",
    "    mag_windowed = mag * window\n",
    "    \n",
    "    # Calculate PSD using periodogram\n",
    "    f, psd_mag = signal.welch(mag_windowed, fs, nperseg=len(time_seconds)/w)\n",
    "    _, psd_x = signal.welch(x_windowed, fs, nperseg=len(time_seconds)/w)\n",
    "    _, psd_y = signal.welch(y_windowed, fs, nperseg=len(time_seconds)/w)\n",
    "    _, psd_z = signal.welch(z_windowed, fs, nperseg=len(time_seconds)/w)\n",
    "\n",
    "    return f, psd_mag, psd_x, psd_y, psd_z\n",
    "\n",
    "\n",
    "# Time_domain plot\n",
    "def time_domain(num):\n",
    "    df = dataframes[num]\n",
    "\n",
    "    fig = px.line(df, x='Time', y='Magnitude', title='Vibration Sensor Data', labels={'Magnitude': 'Acceleration'})\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=600,\n",
    "        showlegend=True,\n",
    "        paper_bgcolor='darkgrey'\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "\n",
    "\n",
    "#Plotting PSD\n",
    "def freq_domain(frequencies, psd_mag, psd_x, psd_y, psd_z):\n",
    "    # Convert complex PSD values to magnitude\n",
    "    psd_mag_mag = np.abs(psd_mag)\n",
    "    psd_x_mag = np.abs(psd_x)\n",
    "    psd_y_mag = np.abs(psd_y)\n",
    "    psd_z_mag = np.abs(psd_z)\n",
    "    # Create line plots for X, Y, and Z axes\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=frequencies, y=psd_mag_mag, mode='lines', name='Magnitude'))\n",
    "    fig.add_trace(go.Scatter(x=frequencies, y=psd_x_mag, mode='lines', name='X'))\n",
    "    fig.add_trace(go.Scatter(x=frequencies, y=psd_y_mag, mode='lines', name='Y'))\n",
    "    fig.add_trace(go.Scatter(x=frequencies, y=psd_z_mag, mode='lines', name='Z'))\n",
    "    fig.update_layout(\n",
    "        title='Power Spectral Density',\n",
    "        xaxis=dict(title='Frequency'),\n",
    "        yaxis=dict(title='acceleration', type='log'),\n",
    "        showlegend=True,\n",
    "        paper_bgcolor= 'darkgrey',\n",
    "        height=600\n",
    "\n",
    "\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62d5de4-3fde-4449-8c58-02131c567548",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calling the function with the DataFrames\n",
    "for j in range(5):\n",
    "    for i in range(j, 60, 20):\n",
    "        print(i)\n",
    "        print(exp.iloc[i])\n",
    "        new = time_domain(i) \n",
    "        experiment, mag, x, y, z = calculate_psd(dataframes[i], fs=1000)\n",
    "        plo = freq_domain(experiment, mag, x, y, z)\n",
    "        \n",
    "        print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615b3b62-edd6-4b98-be49-b9f0280a8c50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute the spectrogram using STFT\n",
    "frequencies, times, spectrogram = signal.spectrogram(merged_df['Magnitude'], fs=1000)\n",
    "\n",
    "# Plot the time-frequency representation\n",
    "plt.pcolormesh(times, frequencies, 10 * np.log10(spectrogram), shading='auto', cmap='inferno')\n",
    "\n",
    "# Configure the plot\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Magnitude Time-Frequency Plot')\n",
    "plt.colorbar(label='Power Spectral Density (dB)')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2c0017-bacf-4fe5-89cf-4476c134ef4f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fc8adf-05a6-47ef-98ba-7456664c57da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af64b146-f02e-47e0-a209-b4729eaeec60",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1153688-983a-400a-9b70-5a56d6f975d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Prepare the Data\n",
    "# Split the data into features (X) and target variable (y)\n",
    "columns_to_drop = ['Ra', 'X', 'Y', 'Z', 'Time', 'Magnitude']\n",
    "X = merged_df.drop(columns_to_drop, axis=1)\n",
    "y = merged_df['Ra']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4438d014-6838-432f-90ce-4112fca8ca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Shuffle the test data sets correspondingly\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998bfe88-c4a9-43ce-b9fe-365ec3effa9a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffe153c-082c-4114-8bf4-939f984b10ab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# USING CPU COMPUTATION\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 1. Data Preparation - Assume X and y contain the features and target variable\n",
    "\n",
    "# 2. Import Libraries\n",
    "\n",
    "# 4. Create Random Forest Model\n",
    "model = RandomForestRegressor(n_estimators=100, max_depth=10)\n",
    "\n",
    "# 5. Fit the Model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 6. Make Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 7. Evaluate the Model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print('Mean Squared Error:', mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a212b7c-cff2-416d-aa22-553af47ab657",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# USING GPU COMPUTATION\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 3. Create XGBoost Random Forest Model\n",
    "model = XGBRegressor(n_estimators=300, tree_method='gpu_hist')\n",
    "\n",
    "# 4. Fit the Model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 5. Make Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 6. Evaluate the Model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print('Mean Squared Error:', mse)\n",
    "\n",
    "# Calculate the MAE\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "# Print the MAE\n",
    "print('Mean Absolute Error:', mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0b5e97-52f7-40bf-9db8-8de95b682e36",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model.save_model('xgboost_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d95dfc-0703-4110-b594-fdc65f4c5eb2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame with 'y_pred' and 'y_train' columns\n",
    "comparison_df = pd.DataFrame({'y_pred': y_pred, 'y_test': y_test})\n",
    "\n",
    "# Print the DataFrame\n",
    "comparison_df.head(30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0e1f1e-5cf2-4327-bc2c-5deae716c862",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9706c301-34fe-4eaf-adb9-621c4d17a3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "from cuml import SVR\n",
    "\n",
    "# 1. Transfer data to GPU\n",
    "X_train_gpu = cp.asarray(X_train)\n",
    "y_train_gpu = cp.asarray(y_train)\n",
    "X_test_gpu = cp.asarray(X_test)\n",
    "\n",
    "# 2. Create the GPU-based SVM model\n",
    "model_gpu = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "\n",
    "# 3. Fit the model\n",
    "model_gpu.fit(X_train_gpu, y_train_gpu)\n",
    "\n",
    "# 4. Make predictions\n",
    "y_pred_gpu = model_gpu.predict(X_test_gpu)\n",
    "\n",
    "# 5. Transfer predictions back to CPU\n",
    "y_pred_cpu = cp.asnumpy(y_pred_gpu)\n",
    "\n",
    "# 6. Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred_cpu)\n",
    "print('Mean Squared Error:', mse)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred_cpu)\n",
    "print('Mean Absolute Error:', mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2046eb9-f3bd-4c5d-a788-710c11c8ac44",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec27b605-6854-41b0-8b52-8f9adae78976",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Using Keras-Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07ece09-ab91-471b-bce0-e6795909113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow import keras\n",
    "import keras_tuner as kt\n",
    "\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# Scale the features\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the model-building function\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Conv1D(hp.Int('conv_units', min_value=64, max_value=2048, step=64),\n",
    "                                  kernel_size=2, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(keras.layers.MaxPooling1D(pool_size=2))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(hp.Int('dense_units', min_value=256, max_value=6400, step=256),\n",
    "                                 activation='relu'))\n",
    "    model.add(keras.layers.Dense(960, activation='linear'))\n",
    "\n",
    "    optimizer = hp.Choice('optimizer', values=['Adam', 'sgd', 'adadelta', 'rmsprop'])\n",
    "    #optimizer = Adam(learning_rate=hp.Choice('learning_rate', values=[0.001, 0.00001, 0.00001]))\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbad2bc-18ff-46de-8d90-18f225d195ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tuner\n",
    "tuner = kt.RandomSearch(build_model, objective='val_loss', max_trials=100, directory='tuner_results')\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "tuner.search(np.expand_dims(X_train_scaled, axis=-1), y_train, epochs=4, batch_size=5760, validation_data=(np.expand_dims(X_test_scaled, axis=-1), y_test))\n",
    "\n",
    "# Get the best model\n",
    "best_model = tuner.get_best_models(1)[0]\n",
    "\n",
    "# Evaluate the best modesl\n",
    "loss, mae = best_model.evaluate(np.expand_dims(X_test_scaled, axis=-1), y_test)\n",
    "print('Mean Squared Error:', loss)\n",
    "print('Mean Absolute Error:', mae)\n",
    "# Make predictions\n",
    "predictions = best_model.predict(np.expand_dims(X_test_scaled, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a9ada9-cb86-4afd-9b80-536072d8ae81",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_model.predict(np.expand_dims(X_test_scaled, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f28ca7-1f6d-4a30-9f36-c56ad1642cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281fcc73-c4be-47c1-bc61-541cc80f7e79",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Manual-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6809e5f0-6479-40e2-a964-4b957e53b2c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "#Scale the features\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#the CNN Model\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Conv1D(2048, kernel_size=1, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(960, activation='linear'))\n",
    "\n",
    "optimizer = Adam(learning_rate=0.0005)\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mae','accuracy'])\n",
    "\n",
    "# Train the CNN Model\n",
    "model.fit(np.expand_dims(X_train_scaled, axis=-1), y_train, epochs=8, batch_size=5760)\n",
    "\n",
    "# Evaluate the CNN Model\n",
    "loss, mae = model.evaluate(np.expand_dims(X_test_scaled, axis=-1), y_test)\n",
    "print('Mean Squared Error:', loss)\n",
    "print('Mean Absolute Error:', mae)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(np.expand_dims(X_test_scaled, axis=-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87973e07-2124-4144-bb1c-798b856b1ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import save_model\n",
    "\n",
    "# Save the model\n",
    "best_model.save('cnn_tuner.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cbe5c5-e8c1-4428-9753-c0bafcd3be53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "s_model = load_model('cnn_tuner.h5')\n",
    "s_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfe263b-16ea-4abb-9c6a-eec9b4a33962",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions1 = s_model.predict(np.expand_dims(X_test_scaled, axis=-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9287934-0b9d-49f2-ba3b-7d96cf936d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc70ea5f-d890-49d2-a60d-839afcf5e9e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
